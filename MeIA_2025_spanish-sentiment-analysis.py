# -*- coding: utf-8 -*-
"""MeIA_2025_Analisis_de_sentimientos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12EzdP9-S3HGZWqv2vtCPTWdMW-4wVZYT

# Lectura del corpus de entrenamiento
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
data = pd.read_excel('/content/drive/MyDrive/Curso IA - MeIA/datos/MeIA_2025_train.xlsx', sheet_name="MeIA_2025_train")
#Toma en cuenta que debes cambiar la ruta del archivo de tal modo que coincida con la ubicación
#de tu archivo
data['Polarity'] = data['Polarity'].astype(int) #para convertir las clases a tipo entero
data = data.drop(columns=['Town','Region','Type']) #solo si quieres quitar estas columnas
data = data.rename(columns={'Review': 'texto'}) #para estandarizar con los modelos de transfomers
data = data.rename(columns={'Polarity': 'labels'}) #para estandarizar con los modelos de transfomers
data['labels'] = data['labels'] - 1 #para que las clases estén de en el rango de [0-4]

print("Dimensiones del dataset de entrenamiento:", data.shape)
data.head()

"""#Para abrir archivo de test"""

pf_test = pd.read_excel('/content/drive/MyDrive/Curso IA - MeIA/datos/MeIA_2025_test_wo_labels.xlsx', sheet_name="MeIA_2025_test_wo_labels")
pf_test['labels'] = data['labels']  # si 'Polarity' existe
pf_test = pf_test.rename(columns={'Review': 'texto'})#para estandarizar con los modelos de transfomers
pf_test = pf_test.drop(columns=['Town','Region','Type', 'ID'], errors='ignore')#solo si quieres quitar estas columnas
pf_test.head()

"""# diccionarios de tus clases

**----------------------------------------------MI CODIGO----------------------------------------------**
"""

! nvidia-smi

#!pip install datasets
#!pip install transformers==4.28.0
#!pip install --upgrade accelerate

import re

def limpiar_ruido(texto):
    # Pasa a minúsculas para detectar patrones
    texto = texto.lower()

    # Elimina fragmentos típicos de truncamiento
    texto = re.sub(r'ver\s*más', '', texto)
    texto = re.sub(r'leer\s*más', '', texto)
    texto = re.sub(r'https?://\S+', '', texto)  # Quitar URLs
    texto = re.sub(r'[^\w\sáéíóúüñ]', '', texto)  # Eliminar puntuación innecesaria


    # También puedes eliminar caracteres raros o múltiples espacios
    texto = re.sub(r'\s+', ' ', texto).strip()

    return texto

data['texto'] = data['texto'].apply(limpiar_ruido)

data = data[data['texto'].str.strip() != '']
data = data.drop_duplicates(subset='texto')

from datasets import Dataset, DatasetDict

#Unicamente utilizamos el texto y las demás columnas se eliminan
print(data.head(10),"\n")

#from sklearn.model_selection import train_test_split

#train_df, eval_df = train_test_split(data, test_size=0.2, stratify=data['labels'], random_state=42)

#ds_train = Dataset.from_pandas(data, preserve_index=False)
#ds_test = Dataset.from_pandas(pf_test, preserve_index=False)

#dataset = DatasetDict({
    #'train': ds_train,
    #'test': Dataset.from_pandas(pf_test)
#})

from sklearn.model_selection import train_test_split
df_train, df_val = train_test_split(data, test_size=0.2, stratify=data["labels"], random_state=42)

ds_train = Dataset.from_pandas(df_train, preserve_index=False)
ds_val = Dataset.from_pandas(df_val, preserve_index=False)

dataset = DatasetDict({
    'train': ds_train,
    'validation': ds_val
})

id2label = {0: "very negative", 1: "negative", 2: "neutral", 3: "positive", 4: "very positive"}
label2id = {"very negative": 0, "negative": 1, "neutral": 2, "positive": 3, "very positive": 4}

dataset

#model_checkpoint = "dccuchile/bert-base-spanish-wwm-cased" #BUENO (MEJOR DE MOMENTO)
#model_checkpoint = "finiteautomata/beto-sentiment-analysis" #BUENO
#model_checkpoint = "pysentimiento/bert-base-spanish-uncased-sentiment"
#model_checkpoint = "mrm8488/bert-spanish-cased-finetuned-sentiment"
#model_checkpoint = "nlptown/bert-base-multilingual-uncased-sentiment" #BUENO
model_checkpoint = "VerificadoProfesional/SaBERT-Spanish-Sentiment-Analysis" #PROFESINAL - PROMETE MUCHO
#model_checkpoint = "ignacio-ave/beto-sentiment-analysis-spanish" #DE LO MEJOR

from transformers import AutoTokenizer
from transformers import BertForSequenceClassification, BertTokenizerFast

tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint)

def tokenize_fn(examples):
    return tokenizer(
        examples["texto"],
        padding="max_length",  # o True si usas DataCollator
        truncation=True,
        max_length=128
    )

tokenized_dataset = dataset.map(tokenize_fn, batched=True, remove_columns=["texto"])
tokenized_dataset

tokenized_dataset["train"][1]

from transformers import BertForSequenceClassification, BertTokenizerFast

model = BertForSequenceClassification.from_pretrained(
    model_checkpoint,
    num_labels=len(id2label),
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True
)

#!pip install evaluate

from transformers import DataCollatorWithPadding
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import numpy as np
from evaluate import load

# Se encarga del padding dinámico
data_collator = DataCollatorWithPadding(tokenizer)

accuracy_metric = load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds, average="weighted"),
        "precision": precision_score(labels, preds, average="weighted"),
        "recall": recall_score(labels, preds, average="weighted"),
    }

from transformers import TrainingArguments

#training_args = TrainingArguments(
    #output_dir="bert-classifier",
    #evaluation_strategy="epoch",
    #save_strategy="epoch",
    #learning_rate=2e-5,
    #weight_decay=0.01,
    #per_device_train_batch_size=16,
    #per_device_eval_batch_size=16,
    #num_train_epochs=3,
    #load_best_model_at_end=True,
    #remove_unused_columns=False,
    #metric_for_best_model="eval_accuracy",
#)
training_args = TrainingArguments(
    output_dir="bert-classifier",
    evaluation_strategy="steps",
    save_strategy="steps",
    eval_steps=200,
    save_steps=200,
    logging_steps=50,
    logging_dir="./logs",
    learning_rate=3e-5,
    warmup_steps=200,
    weight_decay=0.01,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=8,
    load_best_model_at_end=True,
    remove_unused_columns=False,
    metric_for_best_model="eval_f1",
)

# Verifica el rango
print("Valores únicos de labels:", data['labels'].unique())
assert data['labels'].between(0, 4).all(), "Error: Hay etiquetas fuera del rango permitido (0-4)"

data['labels'].value_counts()

from transformers import Trainer, EarlyStoppingCallback

ignore_mismatched_sizes=True

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

trainer.train()

"""# Pasar test por el pipeline y escribir archivo de evaluación según el formato del MeIA"""

from transformers import pipeline

clf_pipe = pipeline(
    "text-classification",
    model=trainer.model,#Este modelo y tokenizador deben ser los que tú entrenaste
    tokenizer=tokenizer,
    device=-1  # cambia a -1 si usas CPU
)

frases_test = pf_test['texto'].tolist();
batch = clf_pipe(frases_test, padding = True, truncation=True)

for res in batch:
    print(res)

escribir = open('/content/drive/MyDrive/Curso IA - MeIA/ChivaTeam-8.txt', 'w')#recuerda poner la dirección qué tu quieras

i = 0;
for res in batch:
    escribir.write(f"MeIA {i} {label2id[res['label']] + 1}\n")
    i+=1;
escribir.close()